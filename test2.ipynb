{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updates\n",
    "- Pytesseract attempt (done 23/6/2023)\n",
    "- [Perspective Transformation (perspective)](https://towardsdatascience.com/perspective-versus-affine-transformation-25033cef5766) (done 27/6/2023)\n",
    "- update codebase for ocrimg and dashboard\n",
    "- further generalize and automate the image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import imutils\n",
    "from sklearn.metrics import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate the document height and width\n",
    "doc_w, doc_h = 1098, 648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper code\n",
    "def gammaCorrection(src, gamma):\n",
    "    invGamma = 1 / gamma\n",
    "\n",
    "    table = [((i / 255) ** invGamma) * 255 for i in range(256)]\n",
    "    table = np.array(table, np.uint8)\n",
    "\n",
    "    return cv2.LUT(src, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding(img):\n",
    "    '''add padding and return image'''\n",
    "    ratio = img.shape[0] / img.shape[1]\n",
    "    orig = img.copy()\n",
    "    image = imutils.resize(img, height = 500)\n",
    "    \n",
    "    \n",
    "    #top = int(0.05 * image.shape[0]) # shape[0] = rows\n",
    "    top = int(0.1 * doc_h) # shape[0] = rows\n",
    "    bottom = top\n",
    "    #left = int(0.05 * image.shape[1]) # shape[1] = cols\n",
    "    left = int(0.1 * doc_w) # shape[1] = cols\n",
    "    right = left\n",
    "    \n",
    "    #image = resize_with_pad(img, (1100, 648), (0, 0, 0))\n",
    "    image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "    cv2.imshow(\"image with padding\", image)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def get_edges(img):\n",
    "    '''input an image opened by invoking cv2.imread.\n",
    "        output the same image reduced to its edges.'''\n",
    "    # convert the image to grayscale, blur it, and find edges\n",
    "    # in the image\n",
    "    gray = cv2.cvtColor(gammaCorrection(img, 1.05), cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edged = cv2.Canny(gray, 70, 150)\n",
    "    \n",
    "    # show the original image and the edge detected image\n",
    "    #plt.imshow(edged)\n",
    "    \n",
    "    return edged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_boxes(edged):\n",
    "\tcnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\tcnts = imutils.grab_contours(cnts)\n",
    "\tcnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    " \n",
    "\t# loop over the contours\n",
    "\tfor c in cnts:\n",
    "\t\t# approximate the contour\n",
    "\t\tperi = cv2.arcLength(c, True)\n",
    "\t\tapprox = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "\t\t# if our approximated contour has four points, then we\n",
    "\t\t# can assume that we have found our screen\n",
    "\t\tif len(approx) >= 4:\n",
    "\t\t\tscreenCnt = approx\n",
    "\t\t\tbreak\n",
    "\n",
    "\treturn screenCnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_coords(screenCnt):\n",
    "    print(\"coordinate sets\")\n",
    "    print(screenCnt)\n",
    "    screenCnt = screenCnt.reshape(len(screenCnt), 2)\n",
    "    close_pairs = np.array(np.where(~((euclidean_distances(screenCnt, screenCnt) > 50) | (euclidean_distances(screenCnt, screenCnt) == 0)))).T\n",
    "\n",
    "    pair_cleaned = []\n",
    "\n",
    "    for p1, p2 in close_pairs:\n",
    "        a = False\n",
    "        for x1, x2 in pair_cleaned:\n",
    "            if p1 == x2:\n",
    "                a = not a\n",
    "                break\n",
    "        if not a:\n",
    "            pair_cleaned.append((p1, p2))\n",
    "            \n",
    "    print(pair_cleaned)\n",
    "\n",
    "    remove_points = np.array(pair_cleaned)[:, 1]\n",
    "\n",
    "    mask = np.ones(len(screenCnt), dtype=bool) \n",
    "    mask[remove_points] = False\n",
    "    result = screenCnt[mask]\n",
    "\n",
    "    #sorted_points = np.array(sorted(result, key=lambda x: x[0]))\n",
    "    left, right, top, bottom = min(result[:, 0]), max(result[:, 0]), min(result[:, 1]), max(result[:, 1])\n",
    "    print(left, right, top, bottom)\n",
    "\n",
    "    topleft = min(result, key=lambda x: euclidean_distances([x], [(left, top)]))\n",
    "    topright = min(result, key=lambda x: euclidean_distances([x], [(right, top)]))\n",
    "    bottomleft = min(result, key=lambda x: euclidean_distances([x], [(left, bottom)]))\n",
    "    bottomright = min(result, key=lambda x: euclidean_distances([x], [(right, bottom)]))\n",
    "    \n",
    "    return topleft, topright, bottomleft, bottomright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_coords(screenCnt):\n",
    "    screenCnt = screenCnt.reshape(len(screenCnt), 2)\n",
    "    screenCnt[:, 0] -= int(0.1 * doc_w)\n",
    "    screenCnt[:, 1] -= int(0.1 * doc_h)\n",
    "    close_pairs = np.array(np.where(~((euclidean_distances(screenCnt, screenCnt) > 10) | (euclidean_distances(screenCnt, screenCnt) == 0)))).T\n",
    "\n",
    "    if len(close_pairs):\n",
    "        pair_cleaned = []\n",
    "\n",
    "        for p1, p2 in close_pairs:\n",
    "            a = False\n",
    "            for x1, x2 in pair_cleaned:\n",
    "                if p1 == x2:\n",
    "                    a = not a\n",
    "                    break\n",
    "            if not a:\n",
    "                pair_cleaned.append((p1, p2))\n",
    "\n",
    "        remove_points = np.array(pair_cleaned)[:, 1]\n",
    "\n",
    "        mask = np.ones(len(screenCnt), dtype=bool) \n",
    "        mask[remove_points] = False\n",
    "        screenCnt = screenCnt[mask]\n",
    "\n",
    "    #sorted_points = np.array(sorted(screenCnt, key=lambda x: x[0]))\n",
    "    left, right, top, bottom = min(screenCnt[:, 0]), max(screenCnt[:, 0]), min(screenCnt[:, 1]), max(screenCnt[:, 1])\n",
    "    print(left, right, top, bottom)\n",
    "\n",
    "    topleft = min(screenCnt, key=lambda x: euclidean_distances([x], [(left, top)]))\n",
    "    topright = min(screenCnt, key=lambda x: euclidean_distances([x], [(right, top)]))\n",
    "    bottomleft = min(screenCnt, key=lambda x: euclidean_distances([x], [(left, bottom)]))\n",
    "    bottomright = min(screenCnt, key=lambda x: euclidean_distances([x], [(right, bottom)]))\n",
    "    \n",
    "    return topleft, topright, bottomleft, bottomright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(img):\n",
    "    '''the official driver code for the whole process.\n",
    "    Accepts an image as input, for OCRImg class\n",
    "    Outputs the image required for pyt.image_to_string() call to OCR Engine.\n",
    "    Assume that the image has already be resized'''\n",
    "    \n",
    "    #create aspect ratio for ID cards. can be changed for other documents\n",
    "    max_w, max_h = 1098, 648\n",
    "    \n",
    "    #read the image and get its edges\n",
    "    #image = cv2.resize(img, (max_w, max_h), interpolation = cv2.INTER_AREA)\n",
    "    padded = get_padding(img)\n",
    "    edged = get_edges(padded)    \n",
    "    boxes = get_all_boxes(edged)\n",
    "    topleft, topright, bottomleft, bottomright = select_coords(boxes) #something sus here\n",
    "    #print(topleft, topright, bottomleft, bottomright)\n",
    "\n",
    "    #initialize points: input convert to top-down output\n",
    "    input_pts = np.float32([list(coord) for coord in [topleft, bottomleft, bottomright, topright]])\n",
    "    output_pts = np.float32([[0, 0],\n",
    "                            [0, max_h - 1],\n",
    "                            [max_w - 1, max_h - 1],\n",
    "                            [max_w - 1, 0]])\n",
    "    \n",
    "    #plt.imshow(cv2.drawContours(img, [np.array([topleft, topright, bottomright, bottomleft]).reshape((4, 1, 2))], -1, (255,0,0), 2))\n",
    "    #cv2.imshow(\"edged\", edged)\n",
    "    #cv2.waitKey(0)\n",
    "    \n",
    "    # Compute the perspective transform mat\n",
    "    transform_mat = cv2.getPerspectiveTransform(input_pts, output_pts)\n",
    "    out = cv2.warpPerspective(img, transform_mat, (max_w, max_h), flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import json\n",
    "\n",
    "def extract_text(img_path, lang):\n",
    "    return pytesseract.image_to_string(Image.open(img_path), lang = lang)\n",
    "\n",
    "def extract_name(ocr_result):\n",
    "    # Define pattern to extract company\n",
    "    company_pattern = r'(?i)(?:^|\\b)([A-Z][A-Za-z]+(?: [A-Z][A-Za-z]+)*)(?:\\b|$)'\n",
    "\n",
    "    # Extract company using regular expression pattern\n",
    "    company_match = re.search(company_pattern, ocr_result)\n",
    "    if company_match:\n",
    "        return company_match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_email(ocr_result):\n",
    "    # Extract email address using regular expression pattern\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    email_match = re.search(email_pattern, ocr_result, re.I)\n",
    "    if email_match:\n",
    "        return email_match.group()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_company(ocr_result):\n",
    "    # Define pattern to extract company\n",
    "    company_pattern = r'(?i)(?:^|\\b)([A-Z][A-Za-z]+(?: [A-Z][A-Za-z]+)*)(?:\\b|$)'\n",
    "\n",
    "    # Extract company using regular expression pattern\n",
    "    company_match = re.search(company_pattern, ocr_result)\n",
    "    if company_match:\n",
    "        return company_match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_contact(ocr_result):\n",
    "    # Extract contact number using regular expression pattern\n",
    "    contact_number_pattern = r'\\b\\d+\\b'\n",
    "    contact_number_matches = re.findall(contact_number_pattern, ocr_result)\n",
    "    contact_number = ''.join(contact_number_matches)\n",
    "    if contact_number:\n",
    "        return contact_number\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# function to produce the json\n",
    "def extract_json(img_path, lang):\n",
    "    res_dict = {}\n",
    "    \n",
    "    extracted_txt = extract_text(img_path, lang = lang)\n",
    "    res_dict[\"name\"], res_dict[\"email\"], res_dict[\"contact\"], res_dict[\"company\"] = extract_name(extracted_txt), extract_email(extracted_txt), extract_contact(extracted_txt), extract_contact(extracted_txt)\n",
    "    \n",
    "    res_json = json.dumps(res_dict)\n",
    "    return res_json\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver code\n",
    "\n",
    "This is the test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 482 0 499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"name\": \"Alexis Beckman\", \"email\": \"info@culturefrozenyogurt.com\", \"contact\": \"6503240440650324044834094306100\", \"company\": \"6503240440650324044834094306100\"}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = \"./test/099.jpg_Reference.jpg\"\n",
    "\n",
    "img = cv2.imread(test_path)\n",
    "test_img = imutils.resize(img, height = 500)\n",
    "new_img = transform_image(test_img)\n",
    "\n",
    "cv2.imshow(\"Test case\", new_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "pytesseract.image_to_string(new_img)\n",
    "\n",
    "# testing the json function\n",
    "extract_json(test_path, \"eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow(\"normal\", img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
