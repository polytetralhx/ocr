{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updates\n",
    "- Pytesseract attempt (done 23/6/2023)\n",
    "- [Perspective Transformation (perspective)](https://towardsdatascience.com/perspective-versus-affine-transformation-25033cef5766) (done 27/6/2023)\n",
    "- update codebase for ocrimg and dashboard\n",
    "- further generalize and automate the image preprocessing\n",
    "\n",
    "- take note of python version, environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from imutils import resize, grab_contours\n",
    "from sklearn.metrics import euclidean_distances\n",
    "import regex as re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate the document height and width\n",
    "doc_w, doc_h = 1098, 648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper code\n",
    "def gammaCorrection(src, gamma):\n",
    "    invGamma = 1 / gamma\n",
    "\n",
    "    table = [((i / 255) ** invGamma) * 255 for i in range(256)]\n",
    "    table = np.array(table, np.uint8)\n",
    "\n",
    "    return cv2.LUT(src, table)\n",
    "\n",
    "def grayscale(src):\n",
    "    gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n",
    "    return gray\n",
    "\n",
    "def median_blur(src):\n",
    "    return cv2.medianBlur(src, 7)\n",
    "\n",
    "def normalize(src):\n",
    "    norm_img = np.zeros((src.shape[0], src.shape[1]))\n",
    "    img = cv2.normalize(src, norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    return img\n",
    "\n",
    "def contrast_enhance(src):\n",
    "    return cv2.equalizeHist(src)\n",
    "\n",
    "def remove_noise(src):\n",
    "    inter =  cv2.fastNlMeansDenoising(src, None, 10, 10, 7)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    return cv2.morphologyEx(inter, cv2.MORPH_CLOSE, kernel, iterations = 1)\n",
    "\n",
    "def gaussian_kernel(size, sigma=1):\n",
    "    size = int(size) // 2\n",
    "    x, y = np.mgrid[-size:size+1, -size:size+1]\n",
    "    normal = 1 / (2.0 * np.pi * sigma**2)\n",
    "    g =  np.exp(-((x**2 + y**2) / (2.0*sigma**2))) * normal\n",
    "    return g\n",
    "\n",
    "def thresholding(src):\n",
    "    return cv2.threshold(src, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) [1]\n",
    "\n",
    "def non_max_suppression(src, D):\n",
    "    '''makes edges dected in the canny image thinner'''\n",
    "    M, N = src.shape\n",
    "    Z = np.zeros((M,N), dtype=np.int32)\n",
    "    angle = D * 180. / np.pi\n",
    "    angle[angle < 0] += 180\n",
    "    \n",
    "    for i in range(1,M-1):\n",
    "        for j in range(1,N-1):\n",
    "            try:\n",
    "                q = 255\n",
    "                r = 255\n",
    "                \n",
    "               #angle 0\n",
    "                if (0 <= angle[i,j] < 22.5) or (157.5 <= angle[i,j] <= 180):\n",
    "                    q = src[i, j+1]\n",
    "                    r = src[i, j-1]\n",
    "                #angle 45\n",
    "                elif (22.5 <= angle[i,j] < 67.5):\n",
    "                    q = src[i+1, j-1]\n",
    "                    r = src[i-1, j+1]\n",
    "                #angle 90\n",
    "                elif (67.5 <= angle[i,j] < 112.5):\n",
    "                    q = src[i+1, j]\n",
    "                    r = src[i-1, j]\n",
    "                #angle 135\n",
    "                elif (112.5 <= angle[i,j] < 157.5):\n",
    "                    q = src[i-1, j-1]\n",
    "                    r = src[i+1, j+1]\n",
    "\n",
    "                if (src[i,j] >= q) and (src[i,j] >= r):\n",
    "                    Z[i,j] = src[i,j]\n",
    "                else:\n",
    "                    Z[i,j] = 0\n",
    "\n",
    "            except IndexError as e:\n",
    "                pass\n",
    "    return Z\n",
    "\n",
    "def adaptative_threshold(src):\n",
    "    '''obtain binary thresholded image of blurred image'''\n",
    "    mask = cv2.adaptiveThreshold(src, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 31, 10)\n",
    "    return mask\n",
    "\n",
    "def bimodal_threshold(src):\n",
    "    _, thres = cv2.threshold(src, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    return thres\n",
    "\n",
    "def hysteresis(src, weak, strong=255):\n",
    "    '''contrast between weak and strong pixels is increased'''\n",
    "    M, N = src.shape  \n",
    "    for i in range(1, M-1):\n",
    "        for j in range(1, N-1):\n",
    "            if (src[i,j] == weak):\n",
    "                try:\n",
    "                    if ((src[i+1, j-1] == strong) or (src[i+1, j] == strong) or (src[i+1, j+1] == strong)\n",
    "                        or (src[i, j-1] == strong) or (src[i, j+1] == strong)\n",
    "                        or (src[i-1, j-1] == strong) or (src[i-1, j] == strong) or (src[i-1, j+1] == strong)):\n",
    "                        src[i, j] = strong\n",
    "                    else:\n",
    "                        src[i, j] = 0\n",
    "                except IndexError as e:\n",
    "                    pass\n",
    "    return src\n",
    "\n",
    "def get_edges(src, threshold = 90):\n",
    "    '''input an image opened by invoking cv2.imread.\n",
    "        output the same image reduced to its edges.'''\n",
    "    # convert the image to grayscale, blur it, and find edges\n",
    "    aspect_ratio = src.shape[1] / src.shape[0]\n",
    "    threshold_height = int(threshold / aspect_ratio)\n",
    "    print(threshold_height)\n",
    "    return cv2.Canny(src, threshold, threshold_height, apertureSize=3)\n",
    "\n",
    "def morph(src):\n",
    "    return cv2.morphologyEx(src, cv2.MORPH_GRADIENT, cv2.getStructuringElement(cv2.MORPH_RECT), iterations = 7)\n",
    "\n",
    "def equalize(src):\n",
    "    # converting to LAB color space\n",
    "    lab= cv2.cvtColor(src, cv2.COLOR_BGR2LAB)\n",
    "    l_channel, a, b = cv2.split(lab)\n",
    "\n",
    "    # Applying CLAHE to L-channel\n",
    "    # feel free to try different values for the limit and grid size:\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l_channel)\n",
    "\n",
    "    # merge the CLAHE enhanced L-channel with the a and b channel\n",
    "    limg = cv2.merge((cl,a,b))\n",
    "\n",
    "    # Converting image from LAB Color model to BGR color spcae\n",
    "    enhanced_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    # Stacking the original image with the enhanced image\n",
    "    result = np.hstack((src, enhanced_img))\n",
    "    return result\n",
    "\n",
    "def remove_glare(img: np.ndarray, brightness: int):\n",
    "    img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    img_lab[:, :, 0] = brightness\n",
    "    return cv2.cvtColor(img_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def close_edges(src):\n",
    "    kernel = np.ones((9, 9), np.uint8)\n",
    "    return cv2.morphologyEx(src, cv2.MORPH_CLOSE, kernel, iterations = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_boxes(edged):\n",
    "\tcnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\tcnts = grab_contours(cnts)\n",
    "\tcnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    " \n",
    "\t# loop over the contours\n",
    "\tfor c in cnts:\n",
    "\t\t# approximate the contour\n",
    "\t\tperi = cv2.arcLength(c, True)\n",
    "\t\tapprox = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "\t\t# if our approximated contour has four points, then we\n",
    "\t\t# can assume that we have found our screen\n",
    "\t\tif len(approx) >= 4:\n",
    "\t\t\tscreenCnt = approx\n",
    "\t\t\tbreak\n",
    "\n",
    "\treturn screenCnt\n",
    "\n",
    "def select_coords(screenCnt):\n",
    "    screenCnt = screenCnt.reshape(len(screenCnt), 2)\n",
    "\n",
    "    #sorted_points = np.array(sorted(screenCnt, key=lambda x: x[0]))\n",
    "    left, right, top, bottom = min(screenCnt[:, 0]), max(screenCnt[:, 0]), min(screenCnt[:, 1]), max(screenCnt[:, 1])\n",
    "\n",
    "    topleft = min(screenCnt, key=lambda x: euclidean_distances([x], [(left, top)]))\n",
    "    topright = min(screenCnt, key=lambda x: euclidean_distances([x], [(right, top)]))\n",
    "    bottomleft = min(screenCnt, key=lambda x: euclidean_distances([x], [(left, bottom)]))\n",
    "    bottomright = min(screenCnt, key=lambda x: euclidean_distances([x], [(right, bottom)]))\n",
    "    \n",
    "    return topleft, topright, bottomleft, bottomright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img, topleft, topright, bottomleft, bottomright):\n",
    "    max_h, max_w = img.shape[:2]\n",
    "\n",
    "    #initialize points: input convert to top-down output\n",
    "    input_pts = np.float32([list(coord) for coord in [topleft, bottomleft, bottomright, topright]])\n",
    "    output_pts = np.float32([[0, 0],\n",
    "                            [0, max_h - 1],\n",
    "                            [max_w - 1, max_h - 1],\n",
    "                            [max_w - 1, 0]])\n",
    "    \n",
    "    # Compute the perspective transform mat\n",
    "    transform_mat = cv2.getPerspectiveTransform(input_pts, output_pts)\n",
    "    out = cv2.warpPerspective(img, transform_mat, (max_w, max_h), flags=cv2.INTER_LINEAR)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(img, lang):\n",
    "    return pytesseract.image_to_string(img, lang = lang)\n",
    "\n",
    "def extract_name(ocr_result):\n",
    "    # Define pattern to extract company\n",
    "    company_pattern = r'(?i)(?:^|\\b)([A-Z][A-Za-z]+(?: [A-Z][A-Za-z]+)*)(?:\\b|$)'\n",
    "\n",
    "    # Extract company using regular expression pattern\n",
    "    company_match = re.search(company_pattern, ocr_result)\n",
    "    if company_match:\n",
    "        return company_match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_email(text):\n",
    "    pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.,-]+\\.[A-Z|a-z]{2,}(?:,[A-Za-z]+(?:\\.[A-Za-z]+)?)*\\b\"\n",
    "    emails = re.findall(pattern, text)\n",
    "    if emails:\n",
    "        return \" - \".join(emails)\n",
    "    return \"\"\n",
    "\n",
    "def extract_company(ocr_text):\n",
    "    patterns = [\n",
    "        r\"(?i)(Pte Ltd|PTE LTD|\\(Pte.\\) Ltd\\.|Co, Ltd|Co\\., Ltd|CO\\. PTE LTD|Ltd|Ltd\\.) Trading\",\n",
    "        r\"(?i)(Pte Ltd|PTE LTD|\\(Pte.\\) Ltd\\.|Co, Ltd|Co\\., Ltd|CO\\. PTE LTD|Ltd|Ltd\\.)\",\n",
    "        r\"[\\w\\s]+\"\n",
    "    ]\n",
    "\n",
    "    company_names = []\n",
    "\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, ocr_text)\n",
    "        for match in matches:\n",
    "            stripped_match = match.strip()\n",
    "            if not any(char.isdigit() for char in stripped_match) and stripped_match != \"\":\n",
    "                ignore_keywords = [\"com\", \"www\", \"website\"]\n",
    "                if not any(keyword.lower() in stripped_match.lower() for keyword in ignore_keywords):\n",
    "                    company_names.append(stripped_match)\n",
    "\n",
    "    return \" - \".join(company_names)\n",
    "\n",
    "def extract_contact(ocr_result):\n",
    "    # Extract contact number using regular expression pattern\n",
    "    patterns = [\n",
    "        r\"\\d{8}\",  # 00000000\n",
    "        r\"\\d{4} \\d{4}\",  # 0000 0000\n",
    "        r\"\\d{2} \\d{4} \\d{4}\",  # 65 0000 0000\n",
    "        r\"\\(\\d{2}\\) \\d{4} \\d{4}\",  # (65) 0000 0000\n",
    "        r\"\\+\\d{2} \\d{4} \\d{4}\",  # +65 0000 0000\n",
    "        r\"\\+\\d{2} \\d{4}-\\d{4}\",  # +65 0000-0000\n",
    "        r\"\\+\\d{2} \\d{8}\",  # +65 00000000\n",
    "        r\"\\+\\(\\d{2}\\) \\d{4} \\d{4}\",  # +(65) 0000 0000\n",
    "        r\"\\+\\(\\d{3}\\) \\d{3} \\d{3} \\d{3}\",  # +(000) 000 000 000\n",
    "        r\"\\+\\d{2}-\\d{2}-\\d{4}-\\d{4}\",  # +00-00-0000-0000\n",
    "        r\"\\+\\d{2}-\\d{3}-\\d{2}-\\d{4}\",  # +00-000-00-0000\n",
    "        r\"\\+\\d{2} \\d-\\d{4}-\\d{4}\"  # +00 0-0000-0000\n",
    "    ]\n",
    "\n",
    "    contact_numbers = []\n",
    "\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, ocr_result)\n",
    "        if matches:\n",
    "            contact_numbers.extend(matches)\n",
    "\n",
    "    return \" - \".join(contact_numbers)\n",
    "    \n",
    "\n",
    "# function to produce the json\n",
    "def extract_json(img, lang):\n",
    "    res_dict = {}\n",
    "    \n",
    "    extracted_txt = extract_text(img, lang = lang)\n",
    "    res_dict[\"name\"], res_dict[\"email\"], res_dict[\"contact\"], res_dict[\"company\"] = extract_name(extracted_txt), extract_email(extracted_txt), extract_contact(extracted_txt), extract_company(extracted_txt)\n",
    "    \n",
    "    res_json = json.dumps(res_dict)\n",
    "    return res_json\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver code\n",
    "\n",
    "This is the test case. We will be conducting the following:\n",
    "\n",
    "- Evaluate the accuracy of the edge detection performed, in this case we have only tried Canny Edge Detection without any Thresholding\n",
    "- Show the final output of performing OCR on the card.\n",
    "\n",
    "The best model will be transferred to the deployment phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"name\": null, \"email\": \"\", \"contact\": \"\", \"company\": \"\"}'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = \"./Images/20230704_151433.jpg\"\n",
    "img = cv2.imread(test_path)\n",
    "\n",
    "# resize an image for better precision\n",
    "img_resize = resize(img, width=500)\n",
    "even = remove_glare(img_resize, 100)\n",
    "#removed = remove_noise(even)\n",
    "edged = get_edges(even)\n",
    "closed = close_edges(edged)\n",
    "cv2.imshow(\"noise removed\", even)\n",
    "\n",
    "topleft, topright, bottomleft, bottomright = select_coords(get_all_boxes(closed))\n",
    "transformed = crop_image(img_resize, topleft, topright, bottomleft, bottomright)\n",
    "#equal = equalize(transformed)\n",
    "\n",
    "cv2.imshow(\"transformed\", transformed)\n",
    "#cv2.imshow(\"equalized\", equal)\n",
    "cv2.waitKey(0)\n",
    "extract_json(transformed, lang = 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
